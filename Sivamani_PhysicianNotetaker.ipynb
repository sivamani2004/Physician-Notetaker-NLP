{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "97f82247",
      "metadata": {
        "id": "97f82247"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "from transformers import pipeline\n",
        "from transformers import AutoConfig\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "from transformers import BertTokenizerFast\n",
        "from collections import Counter\n",
        "import re\n",
        "import json\n",
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "afdf263d",
      "metadata": {
        "id": "afdf263d"
      },
      "source": [
        "REVIEWING MODELS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "6b3dae94",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "6b3dae94",
        "outputId": "871ed31d-566c-4716-c7f7-f3a3c8576fe4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'hazarri/fine-tuned-roberta-sentiment'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "# deBERTa : NER\n",
        "\"Helios9/BioMed_NER\"\n",
        "\n",
        "# DistilBART : Summarization\n",
        "\"Mahalingam/DistilBart-Med-Summary\"\n",
        "\n",
        "# RoBERTa : Sentiment Analysis\n",
        "\"hazarri/fine-tuned-roberta-sentiment\"\n",
        "\n",
        "# GEMINI API for SOAP NOTE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "UM3gJ3bkJGl1",
      "metadata": {
        "id": "UM3gJ3bkJGl1"
      },
      "source": [
        "Load Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "7_qlLWZYJIVE",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7_qlLWZYJIVE",
        "outputId": "1071758c-036b-4685-a524-fbcb969aa2a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded spaCy model: en_core_web_sm\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded NER model: Helios9/BioMED_NER\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded Summarization model: Mahalingam/DistilBart-Med-Summary\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded Sentiment Analysis model: hazarri/fine-tuned-roberta-sentiment\n"
          ]
        }
      ],
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "print(\"Loaded spaCy model: en_core_web_sm\")\n",
        "ner_model = pipeline(\"ner\", model=\"Helios9/BioMED_NER\",tokenizer=\"Helios9/BioMed_NER\", aggregation_strategy=\"simple\")\n",
        "print(\"Loaded NER model: Helios9/BioMED_NER\")\n",
        "summarizer_pipeline = pipeline(\"summarization\", model=\"Mahalingam/DistilBart-Med-Summary\")\n",
        "print(\"Loaded Summarization model: Mahalingam/DistilBart-Med-Summary\")\n",
        "\n",
        "sentiment_model = pipeline(\"sentiment-analysis\", model=\"hazarri/fine-tuned-roberta-sentiment\")\n",
        "print(\"Loaded Sentiment Analysis model: hazarri/fine-tuned-roberta-sentiment\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Review Model's labels"
      ],
      "metadata": {
        "id": "f1zEsuRwJ_NE"
      },
      "id": "f1zEsuRwJ_NE"
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"hazarri/fine-tuned-roberta-sentiment\"\n",
        "# Load the model's configuration\n",
        "config = AutoConfig.from_pretrained(model_name)\n",
        "# The id2label dictionary contains the mapping from output IDs to label names\n",
        "labels = config.id2label\n",
        "\n",
        "# Print all the labels\n",
        "print(f\"All entity labels supported by '{model_name}':\\n\")\n",
        "for i, label in labels.items():\n",
        "    print(f\"ID {i}: {label}\")\n",
        "\n",
        "# Get a simple list of the unique entity types:\n",
        "unique_entity_types = sorted(list(set([label.split('-')[1] for label in labels.values() if '-' in label])))\n",
        "print(f\"\\nUnique entity categories: {unique_entity_types}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LyWYlVilAeKl",
        "outputId": "d7253354-5fc7-4639-c1f4-42e28c502a8d"
      },
      "id": "LyWYlVilAeKl",
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All entity labels supported by 'hazarri/fine-tuned-roberta-sentiment':\n",
            "\n",
            "ID 0: Negative\n",
            "ID 1: Neutral\n",
            "ID 2: Positive\n",
            "\n",
            "Unique entity categories: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "5a26a6d8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5a26a6d8",
        "outputId": "eb07699f-cdc0-4828-bcf6-8a8a2d8a660e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All entity labels supported by 'Helios9/BioMed_NER':\n",
            "\n",
            "ID 0: O\n",
            "ID 1: B-Activity\n",
            "ID 2: I-Activity\n",
            "ID 3: B-Administration\n",
            "ID 4: I-Administration\n",
            "ID 5: B-Age\n",
            "ID 6: I-Age\n",
            "ID 7: B-Area\n",
            "ID 8: I-Area\n",
            "ID 9: B-Biological_attribute\n",
            "ID 10: I-Biological_attribute\n",
            "ID 11: B-Biological_structure\n",
            "ID 12: I-Biological_structure\n",
            "ID 13: B-Clinical_event\n",
            "ID 14: I-Clinical_event\n",
            "ID 15: B-Color\n",
            "ID 16: I-Color\n",
            "ID 17: B-Coreference\n",
            "ID 18: I-Coreference\n",
            "ID 19: B-Date\n",
            "ID 20: I-Date\n",
            "ID 21: B-Detailed_description\n",
            "ID 22: I-Detailed_description\n",
            "ID 23: B-Diagnostic_procedure\n",
            "ID 24: I-Diagnostic_procedure\n",
            "ID 25: B-Disease_disorder\n",
            "ID 26: I-Disease_disorder\n",
            "ID 27: B-Distance\n",
            "ID 28: I-Distance\n",
            "ID 29: B-Dosage\n",
            "ID 30: I-Dosage\n",
            "ID 31: B-Duration\n",
            "ID 32: I-Duration\n",
            "ID 33: B-Family_history\n",
            "ID 34: I-Family_history\n",
            "ID 35: B-Frequency\n",
            "ID 36: I-Frequency\n",
            "ID 37: B-Height\n",
            "ID 38: I-Height\n",
            "ID 39: B-History\n",
            "ID 40: I-History\n",
            "ID 41: B-Lab_value\n",
            "ID 42: I-Lab_value\n",
            "ID 43: B-Mass\n",
            "ID 44: I-Mass\n",
            "ID 45: B-Medication\n",
            "ID 46: I-Medication\n",
            "ID 47: B-Nonbiological_location\n",
            "ID 48: I-Nonbiological_location\n",
            "ID 49: B-Occupation\n",
            "ID 50: I-Occupation\n",
            "ID 51: B-Other_entity\n",
            "ID 52: I-Other_entity\n",
            "ID 53: B-Other_event\n",
            "ID 54: I-Other_event\n",
            "ID 55: B-Outcome\n",
            "ID 56: I-Outcome\n",
            "ID 57: B-Personal_background\n",
            "ID 58: I-Personal_background\n",
            "ID 59: B-Qualitative_concept\n",
            "ID 60: I-Qualitative_concept\n",
            "ID 61: B-Quantitative_concept\n",
            "ID 62: I-Quantitative_concept\n",
            "ID 63: B-Severity\n",
            "ID 64: I-Severity\n",
            "ID 65: B-Sex\n",
            "ID 66: I-Sex\n",
            "ID 67: B-Shape\n",
            "ID 68: I-Shape\n",
            "ID 69: B-Sign_symptom\n",
            "ID 70: I-Sign_symptom\n",
            "ID 71: B-Subject\n",
            "ID 72: I-Subject\n",
            "ID 73: B-Texture\n",
            "ID 74: I-Texture\n",
            "ID 75: B-Therapeutic_procedure\n",
            "ID 76: I-Therapeutic_procedure\n",
            "ID 77: B-Time\n",
            "ID 78: I-Time\n",
            "ID 79: B-Volume\n",
            "ID 80: I-Volume\n",
            "ID 81: B-Weight\n",
            "ID 82: I-Weight\n",
            "\n",
            "Unique entity categories: ['Activity', 'Administration', 'Age', 'Area', 'Biological_attribute', 'Biological_structure', 'Clinical_event', 'Color', 'Coreference', 'Date', 'Detailed_description', 'Diagnostic_procedure', 'Disease_disorder', 'Distance', 'Dosage', 'Duration', 'Family_history', 'Frequency', 'Height', 'History', 'Lab_value', 'Mass', 'Medication', 'Nonbiological_location', 'Occupation', 'Other_entity', 'Other_event', 'Outcome', 'Personal_background', 'Qualitative_concept', 'Quantitative_concept', 'Severity', 'Sex', 'Shape', 'Sign_symptom', 'Subject', 'Texture', 'Therapeutic_procedure', 'Time', 'Volume', 'Weight']\n"
          ]
        }
      ],
      "source": [
        "model_name = \"Helios9/BioMed_NER\"\n",
        "# Load the model's configuration\n",
        "config = AutoConfig.from_pretrained(model_name)\n",
        "# The id2label dictionary contains the mapping from output IDs to label names\n",
        "labels = config.id2label\n",
        "\n",
        "# Print all the labels\n",
        "print(f\"All entity labels supported by '{model_name}':\\n\")\n",
        "for i, label in labels.items():\n",
        "    print(f\"ID {i}: {label}\")\n",
        "\n",
        "# Get a simple list of the unique entity types:\n",
        "unique_entity_types = sorted(list(set([label.split('-')[1] for label in labels.values() if '-' in label])))\n",
        "print(f\"\\nUnique entity categories: {unique_entity_types}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d8b8d3f9",
      "metadata": {
        "id": "d8b8d3f9"
      },
      "source": [
        "# 1. MEDICAL NLP SUMMARIZATION"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21a959bc",
      "metadata": {
        "id": "21a959bc"
      },
      "source": [
        "OPEN TRANSCRIPT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "5ec686a7",
      "metadata": {
        "id": "5ec686a7"
      },
      "outputs": [],
      "source": [
        "with open('transcript.txt', 'r', encoding='utf-8') as file:\n",
        "    TRANSCRIPT = file.read()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21195b4c",
      "metadata": {
        "id": "21195b4c"
      },
      "source": [
        "Get Name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "34592bff",
      "metadata": {
        "id": "34592bff"
      },
      "outputs": [],
      "source": [
        "def run_spacy_ner(text: str, nlp):\n",
        "    \"\"\"Runs a spaCy NER model and returns the patient name.\"\"\"\n",
        "    print(\"Running spaCy (en_core_web_sm) for General NER \\n\")\n",
        "\n",
        "    # check is found name entity has a doctor prefix\n",
        "    def has_doctor_prefix(ent, doc) -> bool:\n",
        "      # Token-based check: up to 2 tokens before entity start\n",
        "        start_tok = ent.start  # token index where the entity starts\n",
        "        prev_text = doc[max(0, start_tok - 2): start_tok].text.lower().strip()\n",
        "        if re.search(r\"(?:^|\\s)(dr\\.?|doctor)\\s*$\", prev_text):\n",
        "            return True\n",
        "\n",
        "        # Char-based fallback: look a few chars before the entity start to catch 'Dr.John' / 'Dr-John'\n",
        "        left_ctx = text[max(0, ent.start_char - 6): ent.start_char].lower()\n",
        "        if re.search(r\"(?:^|[\\s,.-])dr\\.?\\s*$|doctor\\s*$\", left_ctx):\n",
        "            return True\n",
        "\n",
        "        return False\n",
        "\n",
        "    doc = nlp(text)\n",
        "    patient_name = \"\"\n",
        "    for ent in doc.ents:\n",
        "        if ent.label_ == \"PERSON\" and not patient_name and not has_doctor_prefix(ent, doc):\n",
        "            patient_name = ent.text  # Assume the first PERSON found is patient name\n",
        "            break\n",
        "    return patient_name"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7c766da4",
      "metadata": {
        "id": "7c766da4"
      },
      "source": [
        "Get merged entities from Medical NER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "8efe4a55",
      "metadata": {
        "id": "8efe4a55"
      },
      "outputs": [],
      "source": [
        "def run_helios_ner_with_offsets(text, ner_model):\n",
        "    \"\"\"\n",
        "    Runs NER on chunks and adjusts entity indices to be relative to the full text.\n",
        "    \"\"\"\n",
        "    text_chunks = text.split('\\n\\n')\n",
        "    all_ner_results = []\n",
        "    current_offset = 0\n",
        "    print(\"Running Helios9/BioMED_NER for Medical NER \\n \")\n",
        "    for chunk in text_chunks:\n",
        "        if chunk.strip():\n",
        "            chunk_results = ner_model(chunk)\n",
        "            for entity in chunk_results:\n",
        "                entity['start'] += current_offset\n",
        "                entity['end'] += current_offset\n",
        "            all_ner_results.extend(chunk_results)\n",
        "        current_offset += len(chunk) + 2\n",
        "    return all_ner_results\n",
        "\n",
        "def merge_adjacent_entities(entities, text):\n",
        "    \"\"\"\n",
        "    Merges entities that are consecutive (adjacent) and of the same type.\n",
        "    \"\"\"\n",
        "    if not entities:\n",
        "        return []\n",
        "    entities = sorted(entities, key=lambda x: x['start'])\n",
        "    merged_entities = []\n",
        "    current_entity = entities[0]\n",
        "    for i in range(1, len(entities)):\n",
        "        next_entity = entities[i]\n",
        "        text_between = text[current_entity['end']:next_entity['start']]\n",
        "        if (next_entity['entity_group'] == current_entity['entity_group'] and\n",
        "            len(text_between) <= 1):  # Allow for '' or ' ' or '-'\n",
        "            current_entity['word'] = text[current_entity['start']:next_entity['end']]\n",
        "            current_entity['end'] = next_entity['end']\n",
        "            current_entity['score'] = max(current_entity['score'], next_entity['score'])\n",
        "        else:\n",
        "            merged_entities.append(current_entity)\n",
        "            current_entity = next_entity\n",
        "    merged_entities.append(current_entity)\n",
        "    # Remove low confidence score entities with different thresholds per type\n",
        "    # KEYWORD EXTRACTION\n",
        "    merged_entities = [entity for entity in merged_entities\n",
        "                        if entity['score'] >= (0.6 if entity['entity_group'] == \"Sign_symptom\"\n",
        "                                          else 0.4 if entity['entity_group'] == \"Medication\"\n",
        "                                          else 0.7 if entity['entity_group'] == \"Disease_disorder\"\n",
        "                                          else 0.1 if entity['entity_group'] == \"Lab_value\"\n",
        "                                          else 0.5)]\n",
        "    return merged_entities"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d67b6785",
      "metadata": {
        "id": "d67b6785"
      },
      "source": [
        "print merged_entities (debug)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "94bacda2",
      "metadata": {
        "id": "94bacda2"
      },
      "outputs": [],
      "source": [
        "def print_merged_entities(merged_entities):\n",
        "    print(\"\\n Found Merged Entities : \\n \")\n",
        "    for entity in merged_entities:\n",
        "        print(f\"{entity['entity_group']}: {entity['word']} (Score: {entity['score']:.2f})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11524630",
      "metadata": {
        "id": "11524630"
      },
      "source": [
        "KeyWord Extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "6db432e9",
      "metadata": {
        "id": "6db432e9"
      },
      "outputs": [],
      "source": [
        "def extract_medical_keywords(merged_entities, top_k=12):\n",
        "\n",
        "    relevant_keywords = []\n",
        "\n",
        "    # Define which entity types should be included as medical keywords\n",
        "    keyword_entity_types = {\n",
        "        'Disease_disorder',\n",
        "        'Medication',\n",
        "        'Therapeutic_procedure',\n",
        "        'Diagnostic_procedure',\n",
        "        'Dosage'\n",
        "    }\n",
        "\n",
        "    # Extract relevant entities\n",
        "    for entity in merged_entities:\n",
        "        entity_group = entity['entity_group']\n",
        "        entity_word = entity['word'].strip().lower()\n",
        "\n",
        "        # Include primary medical keywords\n",
        "        if entity_group in keyword_entity_types:\n",
        "            if entity_word not in relevant_keywords:\n",
        "                relevant_keywords.append({\n",
        "                    'keyword': entity_word,\n",
        "                    'score': entity['score'],\n",
        "                    'type': entity_group\n",
        "                })\n",
        "\n",
        "    # Sort by clinical importance (score) and remove duplicates\n",
        "    relevant_keywords.sort(key=lambda x: x['score'], reverse=True)\n",
        "\n",
        "    # Return just the keyword strings (top K)\n",
        "    return [kw['keyword'] for kw in relevant_keywords[:top_k]]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5bbef506",
      "metadata": {
        "id": "5bbef506"
      },
      "source": [
        "Filter Negated Entities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "c2350e32",
      "metadata": {
        "id": "c2350e32"
      },
      "outputs": [],
      "source": [
        "def segment_dialogue_turns(transcript_text):\n",
        "    dialogue_turns = []\n",
        "    lines = transcript_text.split('\\n')\n",
        "    current_pos = 0\n",
        "\n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "        if not line:\n",
        "            continue\n",
        "        # Assume lines start with \"Physician:\" or \"Patient:\"\n",
        "        if line.startswith(\"Physician:\"):\n",
        "            speaker = \"Physician\"\n",
        "            utterance = line[len(\"Physician:\"):].strip()\n",
        "        elif line.startswith(\"Patient:\"):\n",
        "            speaker = \"Patient\"\n",
        "            utterance = line[len(\"Patient:\"):].strip()\n",
        "        else:\n",
        "            # Possibly continuation of previous speaker, handle as needed\n",
        "            continue\n",
        "\n",
        "        start = transcript_text.find(line, current_pos)\n",
        "        end = start + len(line)\n",
        "        dialogue_turns.append({\"speaker\": speaker, \"text\": utterance, \"start\": start, \"end\": end})\n",
        "        current_pos = end\n",
        "\n",
        "    return dialogue_turns\n",
        "\n",
        "\n",
        "\n",
        "def filter_negated_entities_spacy(transcript_text, entity_phrases, nlp, dialogue_turns):\n",
        "    \"\"\"\n",
        "    Cross-turn dialogue negation handling.\n",
        "\n",
        "    Args:\n",
        "    - transcript_text (str): Full transcript text.\n",
        "    - entity_phrases (iterable of str): Extracted medical entities to filter.\n",
        "    - nlp: spaCy language model.\n",
        "    - dialogue_turns (list of dict): List of {\"speaker\": ..., \"text\": ..., \"start\": ..., \"end\": ...} dicts.\n",
        "\n",
        "    Returns:\n",
        "    - Set of entity_phrases NOT negated.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    negation_cues = {\"no\", \"not\", \"without\", \"denies\", \"absent\", \"never\", \"none\", \"free of\", \"not present\"}\n",
        "    filtered_entities = set()\n",
        "    doc = nlp(transcript_text)\n",
        "\n",
        "    for entity in entity_phrases:\n",
        "        found_negation = False\n",
        "\n",
        "        # Find all occurrences of the entity phrase (case-insensitive)\n",
        "        for match in re.finditer(re.escape(entity), transcript_text, re.IGNORECASE):\n",
        "            e_start, e_end = match.start(), match.end()\n",
        "            span = doc.char_span(e_start, e_end)\n",
        "            if span is None:\n",
        "                # Unable to map char span to spaCy span, skip this occurrence\n",
        "                continue\n",
        "\n",
        "            # Find dialogue turn containing this entity occurrence\n",
        "            current_turn = None\n",
        "            next_turn = None\n",
        "            for i, turn in enumerate(dialogue_turns):\n",
        "                if turn[\"start\"] <= e_start < turn[\"end\"]:\n",
        "                    current_turn = turn\n",
        "                    if i + 1 < len(dialogue_turns):\n",
        "                        next_turn = dialogue_turns[i+1]\n",
        "                    break\n",
        "\n",
        "            # Check negation cues within the same utterance (intra-turn negation)\n",
        "            if current_turn is not None:\n",
        "                utterance_lower = current_turn[\"text\"].lower()\n",
        "                if any(cue in utterance_lower for cue in negation_cues):\n",
        "                    found_negation = True\n",
        "            else:\n",
        "                # Fallback: check sentence negation if audio turn not found\n",
        "                if any(tok.lower_ in negation_cues for tok in span.sent):\n",
        "                    found_negation = True\n",
        "\n",
        "            # If entity in physician turn and not negated in the same turn,\n",
        "            # check next patient turn for negation (inter-turn negation)\n",
        "            if not found_negation and current_turn is not None:\n",
        "                if current_turn[\"speaker\"].lower() == \"physician\" and next_turn is not None and next_turn[\"speaker\"].lower() == \"patient\":\n",
        "                    next_turn_lower = next_turn[\"text\"].lower()\n",
        "                    if any(cue in next_turn_lower for cue in negation_cues):\n",
        "                        found_negation = True\n",
        "\n",
        "            # For entity in patient turn, also verify negation cues occur BEFORE entity in that utterance\n",
        "            if not found_negation and current_turn is not None and current_turn[\"speaker\"].lower() == \"patient\":\n",
        "                before_entity_text = transcript_text[current_turn[\"start\"]:e_start].lower()\n",
        "                if any(re.search(r'\\b' + re.escape(cue) + r'\\b', before_entity_text) for cue in negation_cues):\n",
        "                    found_negation = True\n",
        "\n",
        "            if found_negation:\n",
        "                break\n",
        "\n",
        "        if not found_negation:\n",
        "            filtered_entities.add(entity)\n",
        "\n",
        "    return filtered_entities\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e33722c",
      "metadata": {
        "id": "2e33722c"
      },
      "source": [
        "Get Transcript Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "964d7a8f",
      "metadata": {
        "id": "964d7a8f"
      },
      "outputs": [],
      "source": [
        "def run_bart_summarizer(text: str, summarizer_pipeline):\n",
        "    print(\"Running BART Summarizer \\n\")\n",
        "    summary = summarizer_pipeline(text, max_length=150, min_length=120, do_sample=False)\n",
        "    return summary[0]['summary_text']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "beda1fe8",
      "metadata": {
        "id": "beda1fe8"
      },
      "source": [
        "Define OUTPUT SETS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "9416880d",
      "metadata": {
        "id": "9416880d"
      },
      "outputs": [],
      "source": [
        "symptoms = set()\n",
        "diagnosis = set()\n",
        "treatments = set()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0f5f083",
      "metadata": {
        "id": "d0f5f083"
      },
      "source": [
        "Get SYMPTOMS Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "c1484d8e",
      "metadata": {
        "id": "c1484d8e"
      },
      "outputs": [],
      "source": [
        "\n",
        "def get_symptoms_set(merged_entities, transcript_text):\n",
        "    i = 0\n",
        "    while i < len(merged_entities):\n",
        "        entity = merged_entities[i]\n",
        "        # Merge biological_structure with adjacent sign_symptom\n",
        "\n",
        "        # If current entity is Biological_structure, check for runs of adjacent Biological_structures\n",
        "        if entity['entity_group'] == \"Biological_structure\":\n",
        "            # Collect adjacent biological structures (including joined by commas or 'and'/'or')\n",
        "            bio_structs_indices = [i]\n",
        "            j = i + 1\n",
        "            while j < len(merged_entities) and merged_entities[j]['entity_group'] == \"Biological_structure\":\n",
        "                bio_structs_indices.append(j)\n",
        "                j += 1\n",
        "            # Check if sign_symptom is immediately before the first Biological_structure\n",
        "            symptom_indices = []\n",
        "            if i - 1 >= 0 and merged_entities[i - 1]['entity_group'] == \"Sign_symptom\" and \\\n",
        "                merged_entities[i]['start'] - merged_entities[i - 1]['end'] <= 3:  # small threshold for punctuation/space\n",
        "                    symptom_indices.append(i - 1)\n",
        "            # Check if sign_symptom is immediately after the last Biological_structure\n",
        "            if j < len(merged_entities) and merged_entities[j]['entity_group'] == \"Sign_symptom\" and \\\n",
        "                merged_entities[j]['start'] - merged_entities[bio_structs_indices[-1]]['end'] <= 3:\n",
        "                    symptom_indices.append(j)\n",
        "            # For each detected sign_symptom before or after, merge all bio structures with it\n",
        "            for sym_idx in symptom_indices:\n",
        "                symptom_word = transcript_text[merged_entities[sym_idx]['start']:merged_entities[sym_idx]['end']].strip().lower()\n",
        "                for bio_idx in bio_structs_indices:\n",
        "                    bio_word = transcript_text[merged_entities[bio_idx]['start']:merged_entities[bio_idx]['end']].strip().lower()\n",
        "                    merged_symptom = f\"{bio_word} {symptom_word}\"\n",
        "                    symptoms.add(merged_symptom)\n",
        "            i = bio_structs_indices[-1] + 1\n",
        "            continue\n",
        "\n",
        "        # If standalone sign_symptom (not part of recent merge)\n",
        "        if entity['entity_group'] == \"Sign_symptom\":\n",
        "            prev_bio = (i > 0 and merged_entities[i-1]['entity_group'] == \"Biological_structure\"\n",
        "                        and merged_entities[i-1]['end'] >= entity['start'] - 1)\n",
        "            if not prev_bio:\n",
        "                symptoms.add(entity['word'].strip().lower())\n",
        "        i += 1\n",
        "    #end of while loop\n",
        "    return symptoms\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d083d4e8",
      "metadata": {
        "id": "d083d4e8"
      },
      "source": [
        "Get DIAGNOSIS Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "24368aea",
      "metadata": {
        "id": "24368aea"
      },
      "outputs": [],
      "source": [
        "def get_diagnosis_set(merged_entities):\n",
        "    # Fill diagnosis set with Disease_disorder entities\n",
        "    for entity in merged_entities:\n",
        "        if entity['entity_group'] == \"Disease_disorder\":\n",
        "            diagnosis.add(entity['word'].strip().capitalize())\n",
        "    return diagnosis"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "205f3124",
      "metadata": {
        "id": "205f3124"
      },
      "source": [
        "Get TREATMENT Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "3b5ff7ca",
      "metadata": {
        "id": "3b5ff7ca"
      },
      "outputs": [],
      "source": [
        "def get_treatment_set(merged_entities, transcript_text):\n",
        "    # Fill TREATMENT set with Medication and Therapeutic_procedure entities\n",
        "    ''' If a numerical-type entity (Lab_value, Dosage, Duration, Frequency) appears\n",
        "        immediately before a Medication or Therapeutic_procedure entity, combine them.'''\n",
        "    quantity_entities = [ent for ent in merged_entities if ent['entity_group'] in {\"Lab_value\", \"Dosage\", \"Duration\", \"Frequency\", \"Detailed_description\"}]\n",
        "    treatment_entities = [ent for ent in merged_entities if ent['entity_group'] in {\"Medication\", \"Therapeutic_procedure\"}]\n",
        "\n",
        "    used_quantities = set() # To prevent using the same quantity twice\n",
        "\n",
        "    for treat_ent in treatment_entities:\n",
        "        found_link = False\n",
        "        # Look for a quantity entity within a 30-character window BEFORE the treatment\n",
        "        window_start = treat_ent['start'] - 30\n",
        "\n",
        "        for quant_ent in quantity_entities:\n",
        "            # Check if the quantity is within the window and hasn't been used yet\n",
        "            if quant_ent['end'] < treat_ent['start'] and quant_ent['start'] >= window_start and id(quant_ent) not in used_quantities:\n",
        "\n",
        "                # Check if the text between them is short (e.g., \" sessions of \")\n",
        "                text_between = transcript_text[quant_ent['end']:treat_ent['start']]\n",
        "                if len(text_between.strip().split()) <= 3:\n",
        "                    combined_phrase = f\"{quant_ent['word']} units of {treat_ent['word']}\"\n",
        "                    treatments.add(combined_phrase.strip().lower())\n",
        "                    used_quantities.add(id(quant_ent)) # Mark this quantity as used\n",
        "                    found_link = True\n",
        "                    break # Found a link, move to the next treatment\n",
        "\n",
        "        # If no quantity was linked, add the treatment by itself\n",
        "        if not found_link:\n",
        "            treatments.add(treat_ent['word'].strip().lower())\n",
        "\n",
        "    # --- End of Treatment Extraction ---\n",
        "    return treatments"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9b2796a",
      "metadata": {
        "id": "e9b2796a"
      },
      "source": [
        "Get CURRENT_STATUS & PROGNOSIS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "12cdacb9",
      "metadata": {
        "id": "12cdacb9"
      },
      "outputs": [],
      "source": [
        "def get_current_status(bart_summary):\n",
        "    patterns = [\n",
        "        r\"(currently.*?)(?:\\.|,|;|$)\",\n",
        "        r\"(still.*?)(?:\\.|,|;|$)\",\n",
        "        r\"(occasional.*?)(?:\\.|,|;|$)\",\n",
        "        r\"(patient is.*?stable.*?)(?:\\.|,|;|$)\",\n",
        "        r\"(symptoms have (improved|resolved|persisted).*?)(?:\\.|,|;|$)\",\n",
        "        r\"(no new symptoms?.*?)(?:\\.|,|;|$)\",\n",
        "        r\"(functioning well.*?)(?:\\.|,|;|$)\",\n",
        "        r\"(able to.*?perform daily activities.*?)(?:\\.|,|;|$)\",\n",
        "        r\"(signs of improvement.*?)(?:\\.|,|;|$)\",\n",
        "        r\"(ongoing treatment.*?)(?:\\.|,|;|$)\"\n",
        "    ]\n",
        "\n",
        "    for p in patterns:\n",
        "        match = re.search(p, bart_summary, re.IGNORECASE)\n",
        "        if match:\n",
        "            return match.group(1).strip().capitalize()\n",
        "    return \"\"\n",
        "\n",
        "def get_prognosis(bart_summary):\n",
        "    patterns = [\n",
        "        r\"(prognosis.*?)(?:\\.|,|;|$)\",\n",
        "        r\"(expect.?full recovery.?)(?:\\.|,|;|$)\",\n",
        "        r\"(full recovery.*?)(?:\\.|,|;|$)\",\n",
        "        r\"(recovery.*?)(?:\\.|,|;|$)\",\n",
        "        r\"(recover.*?)(?:\\.|,|;|$)\",\n",
        "        r\"(no long-term.*?)(?:\\.|,|;|$)\",\n",
        "        r\"(expected to recover.*?)(?:\\.|,|;|$)\",\n",
        "        r\"(likely to recover.*?)(?:\\.|,|;|$)\",\n",
        "        r\"(on track for.*?recovery.*?)(?:\\.|,|;|$)\",\n",
        "        r\"(good prognosis.*?)(?:\\.|,|;|$)\",\n",
        "        r\"(no evidence of.*?long-term damage.*?)(?:\\.|,|;|$)\",\n",
        "        r\"(expected to improve.*?)(?:\\.|,|;|$)\",\n",
        "        r\"(recovery anticipated within.*?)(?:\\.|,|;|$)\",\n",
        "        r\"(no chronic complications.*?)(?:\\.|,|;|$)\"\n",
        "    ]\n",
        "\n",
        "    for p in patterns:\n",
        "        match = re.search(p, bart_summary, re.IGNORECASE)\n",
        "        if match:\n",
        "            return match.group(1).strip().capitalize()\n",
        "    return \"\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e1938c53",
      "metadata": {
        "id": "e1938c53"
      },
      "source": [
        " GENERATE SUMMARY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "d6d3d0a3",
      "metadata": {
        "id": "d6d3d0a3"
      },
      "outputs": [],
      "source": [
        "def summary_generator(transcript_text, merged_entities, patient_name):\n",
        "    diagnosis = get_diagnosis_set(merged_entities)\n",
        "    symptoms = get_symptoms_set(merged_entities, transcript_text)\n",
        "    treatments = get_treatment_set(merged_entities, transcript_text)\n",
        "\n",
        "    bart_summary = run_bart_summarizer(transcript_text, summarizer_pipeline)\n",
        "\n",
        "    current_status = get_current_status(bart_summary)\n",
        "    prognosis = get_prognosis(bart_summary)\n",
        "    medical_keywords_found = extract_medical_keywords(merged_entities, top_k=12)\n",
        "\n",
        "    dialogue_turns = segment_dialogue_turns(TRANSCRIPT)\n",
        "    filtered_diagnosis = filter_negated_entities_spacy(transcript_text, diagnosis, nlp, dialogue_turns)\n",
        "    filtered_symptoms = filter_negated_entities_spacy(transcript_text, symptoms, nlp, dialogue_turns)\n",
        "    filtered_treatments = filter_negated_entities_spacy(transcript_text, treatments, nlp, dialogue_turns)\n",
        "\n",
        "    print(\"\\n GENERATED MEDICAL SUMMARY : \\n\")\n",
        "    summary = {\n",
        "        \"Patient_Name\": patient_name,\n",
        "        \"Symptoms\": [\", \".join(sorted([s.capitalize() for s in filtered_symptoms]))],\n",
        "        \"Diagnosis\": [\", \".join(sorted([d.capitalize() for d in filtered_diagnosis]))],\n",
        "        \"Treatment\": [\", \".join(sorted([t.capitalize() for t in filtered_treatments]))],\n",
        "        \"Current_Status\": current_status,\n",
        "        \"Prognosis\": prognosis,\n",
        "        \"Medical_Keywords\": [\", \".join(sorted([m.capitalize() for m in medical_keywords_found]))],\n",
        "        \"Summary\": bart_summary\n",
        "    }\n",
        "    print(json.dumps(summary, indent=2))\n",
        "    return summary"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5c7f051d",
      "metadata": {
        "id": "5c7f051d"
      },
      "source": [
        "# 2. SENTIMENT ANALYSIS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "a1a6c12f",
      "metadata": {
        "id": "a1a6c12f"
      },
      "outputs": [],
      "source": [
        "def predict_sentiment(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Predicts sentiment label from the Hazarri medical model.\n",
        "    Maps to Anxious/Neutral/Reassured.\n",
        "    \"\"\"\n",
        "    RAW2CLIN = {\n",
        "    \"Negative\": \"Anxious\",\n",
        "    \"Neutral\":  \"Neutral\",\n",
        "    \"Positive\": \"Reassured\"\n",
        "    }\n",
        "    if not text or not text.strip():\n",
        "        return \"Neutral\"\n",
        "    out = sentiment_model(text, truncation=True, max_length=256)\n",
        "    # returns a list of dicts: [{'label': 'Positive', 'score': 0.98}]\n",
        "    label = out[0][\"label\"]\n",
        "    return RAW2CLIN.get(label, \"Neutral\")\n",
        "\n",
        "def detect_intent(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Rule-based intent detection.\n",
        "    \"\"\"\n",
        "    text_low = text.lower()\n",
        "    if re.search(r\"\\b(worried|anxious|concerned|fear|scared)\\b\", text_low):\n",
        "        return \"Expressing concern\"\n",
        "    if re.search(r\"\\b(hope|get better|reassure|okay|improve|recover)\\b\", text_low):\n",
        "        return \"Seeking reassurance\"\n",
        "    if re.search(r\"\\b(pain|ache|hurt|symptom)\\b\", text_low):\n",
        "        return \"Reporting symptoms\"\n",
        "    return \"Neutral/Other\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "id": "067476a7",
      "metadata": {
        "id": "067476a7"
      },
      "outputs": [],
      "source": [
        "def get_sentiment_analysis(transcript_text: str):\n",
        "  # Run sentiment analysis only on patient utterances\n",
        "    dialogue_turns = segment_dialogue_turns(transcript_text)\n",
        "    # get patient utterances from segment dialogue function\n",
        "    patient_utterances = [\n",
        "        turn[\"text\"] for turn in dialogue_turns\n",
        "        if turn[\"speaker\"] == \"Patient\"\n",
        "    ]\n",
        "    # Run sentiment & intent on each utterance\n",
        "    sentiments = []\n",
        "    intents    = []\n",
        "    for utt in patient_utterances:\n",
        "        s = predict_sentiment(utt)\n",
        "        i = detect_intent(utt)\n",
        "        sentiments.append(s)\n",
        "        intents.append(i)\n",
        "\n",
        "    # Aggregate with majority\n",
        "    def majority_vote(labels, neutral_label):\n",
        "        filtered = [lbl for lbl in labels if lbl != neutral_label]\n",
        "        return Counter(filtered).most_common(1)[0][0] if filtered else neutral_label\n",
        "\n",
        "    final_sentiment = majority_vote(sentiments, \"Neutral\")\n",
        "    final_intent    = majority_vote(intents,    \"Neutral/Other\")\n",
        "\n",
        "\n",
        "    print(\"\\n GENERATED SENTIMENT ANALYSIS : \\n\")\n",
        "    sentiment_summary = {\n",
        "        \"Sentiment\": final_sentiment,\n",
        "        \"Intent\":   final_intent\n",
        "    }\n",
        "    print(json.dumps(sentiment_summary, indent=2))\n",
        "    return sentiment_summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "nTHQ2ijErbs3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nTHQ2ijErbs3",
        "outputId": "8a88bb74-9fd4-4b40-9a13-7022d1c400e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-genai in /usr/local/lib/python3.12/dist-packages (1.38.0)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.12/dist-packages (2.11.9)\n",
            "Requirement already satisfied: anyio<5.0.0,>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from google-genai) (4.10.0)\n",
            "Requirement already satisfied: google-auth<3.0.0,>=2.14.1 in /usr/local/lib/python3.12/dist-packages (from google-genai) (2.38.0)\n",
            "Requirement already satisfied: httpx<1.0.0,>=0.28.1 in /usr/local/lib/python3.12/dist-packages (from google-genai) (0.28.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.28.1 in /usr/local/lib/python3.12/dist-packages (from google-genai) (2.32.4)\n",
            "Requirement already satisfied: tenacity<9.2.0,>=8.2.3 in /usr/local/lib/python3.12/dist-packages (from google-genai) (8.5.0)\n",
            "Requirement already satisfied: websockets<15.1.0,>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from google-genai) (15.0.1)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.11.0 in /usr/local/lib/python3.12/dist-packages (from google-genai) (4.15.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic) (0.4.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0.0,>=4.8.0->google-genai) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0.0,>=4.8.0->google-genai) (1.3.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.14.1->google-genai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.14.1->google-genai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.14.1->google-genai) (4.9.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0,>=0.28.1->google-genai) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0,>=0.28.1->google-genai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0,>=0.28.1->google-genai) (0.16.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.28.1->google-genai) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.28.1->google-genai) (2.5.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0,>=2.14.1->google-genai) (0.6.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install google-genai pydantic"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "AotdN6eEDa1A",
      "metadata": {
        "id": "AotdN6eEDa1A"
      },
      "source": [
        "# 3. SOAP NOTE GENERATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "id": "lOhcMlxbDe-a",
      "metadata": {
        "id": "lOhcMlxbDe-a"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from typing import Optional\n",
        "from pydantic import BaseModel, Field\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "\n",
        "# Get your api key\n",
        "\n",
        "os.environ[\"GEMINI_API_KEY\"] = \"AIzaSyCXBi7N6f7GjcJ6Hu8tq_QCTC2mp_j4Szc\"\n",
        "\n",
        "# Define a strict JSON schema using Pydantic (auto-converted to JSON Schema by the SDK)\n",
        "class Subjective(BaseModel):\n",
        "    Chief_Complaint: str = Field(default=\"\")\n",
        "    History_of_Present_Illness: str = Field(default=\"\")\n",
        "\n",
        "class Objective(BaseModel):\n",
        "    Physical_Exam: str = Field(default=\"\")\n",
        "    Observations: str = Field(default=\"\")\n",
        "\n",
        "class Assessment(BaseModel):\n",
        "    Diagnosis: str = Field(default=\"\")\n",
        "    Severity: str = Field(default=\"\")\n",
        "\n",
        "class Plan(BaseModel):\n",
        "    Treatment: str = Field(default=\"\")\n",
        "    Follow_Up: str = Field(default=\"\")\n",
        "\n",
        "class SOAPNote(BaseModel):\n",
        "    Subjective: Subjective\n",
        "    Objective: Objective\n",
        "    Assessment: Assessment\n",
        "    Plan: Plan\n",
        "\n",
        "def generate_soap_with_gemini(transcript_text: str,\n",
        "                              model_name: str = \"gemini-2.5-flash\") -> dict:\n",
        "\n",
        "    # System instruction to constrain behavior\n",
        "    system_instruction = (\n",
        "        \"You are a medical scribe. Use only the provided transcript to produce a concise, \"\n",
        "        \"clinically coherent SOAP note in complete sentences. Do not add extraneous text. \"\n",
        "        \"If a field is not explicitly mentioned, leave it as an empty string.\"\n",
        "    )\n",
        "\n",
        "    client = genai.Client()  # Reads GEMINI_API_KEY from env by default\n",
        "\n",
        "    # Configure strict JSON output bound to the SOAPNote schema\n",
        "    config = types.GenerateContentConfig(\n",
        "        system_instruction=system_instruction,\n",
        "        response_mime_type=\"application/json\",\n",
        "        response_schema=SOAPNote,  # The SDK converts this to JSON Schema\n",
        "        temperature=0.2,\n",
        "    )\n",
        "\n",
        "    # Call the model\n",
        "    response = client.models.generate_content(\n",
        "        model=model_name,\n",
        "        contents=transcript_text,\n",
        "        config=config,\n",
        "    )\n",
        "\n",
        "    # Prefer parsed Pydantic object if available; otherwise parse JSON text\n",
        "    try:\n",
        "        parsed = response.parsed  # -> SOAPNote instance when response_schema is set\n",
        "        if parsed:\n",
        "            # Pydantic v2: model_dump; v1: dict()\n",
        "            return parsed.model_dump()\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    # Fallback to JSON text\n",
        "    return json.loads(response.text)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3jf3mSbD6E0e",
      "metadata": {
        "id": "3jf3mSbD6E0e"
      },
      "source": [
        "# Create pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "l08PHT9uACRU",
      "metadata": {
        "id": "l08PHT9uACRU"
      },
      "source": [
        "save json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "id": "s9d58ED6AEPp",
      "metadata": {
        "id": "s9d58ED6AEPp"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "def save_json(data: dict, output_path: str):\n",
        "    out_file = Path(output_path)\n",
        "    out_file.parent.mkdir(parents=True, exist_ok=True)\n",
        "    with open(out_file, 'w', encoding='utf-8') as f:\n",
        "        json.dump(data, f, ensure_ascii=False, indent=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ZaL096aKBAwn",
      "metadata": {
        "id": "ZaL096aKBAwn"
      },
      "source": [
        "final pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "id": "xglmXAk36IlG",
      "metadata": {
        "id": "xglmXAk36IlG"
      },
      "outputs": [],
      "source": [
        "def run_pipeline(transcript_text):\n",
        "    patient_name = run_spacy_ner(transcript_text, nlp)\n",
        "    #print(f\"Extracted Patient Name: {patient_name}\")\n",
        "\n",
        "    ner_results = run_helios_ner_with_offsets(transcript_text, ner_model)\n",
        "    merged_entities = merge_adjacent_entities(ner_results, transcript_text)\n",
        "    #print_merged_entities(merged_entities)  # Debug print of merged entities\n",
        "\n",
        "    patient_summary = summary_generator(transcript_text, merged_entities, patient_name)\n",
        "    patient_sentiment_intent = get_sentiment_analysis(transcript_text)\n",
        "\n",
        "    print(\"\\n GENERATED SOAP NOTE : \\n\")\n",
        "    soap_json = generate_soap_with_gemini(transcript_text)\n",
        "    print(json.dumps(soap_json, indent=2))\n",
        "\n",
        "    save_json(patient_summary, \"output/patient_summary.json\")\n",
        "    save_json(patient_sentiment_intent, \"output/patient_sentiment_intent.json\")\n",
        "    save_json(soap_json, \"output/patient_soap_summary.json\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "id": "k28aFLMC6JOT",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k28aFLMC6JOT",
        "outputId": "52338c7f-dbdc-4e88-f0dc-b8d99d9f4c18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running spaCy (en_core_web_sm) for General NER \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Helios9/BioMED_NER for Medical NER \n",
            " \n",
            "Running BART Summarizer \n",
            "\n",
            "\n",
            " GENERATED MEDICAL SUMMARY : \n",
            "\n",
            "{\n",
            "  \"Patient_Name\": \"Jones\",\n",
            "  \"Symptoms\": [\n",
            "    \"Back pain, Neck pain, Stiffness\"\n",
            "  ],\n",
            "  \"Diagnosis\": [\n",
            "    \"Whiplash injury\"\n",
            "  ],\n",
            "  \"Treatment\": [\n",
            "    \"Painkillers, Ten units of physiotherapy\"\n",
            "  ],\n",
            "  \"Current_Status\": \"Still gets occasional backaches\",\n",
            "  \"Prognosis\": \"Full recovery within six months of the accident\",\n",
            "  \"Medical_Keywords\": [\n",
            "    \"Mobility, Painkillers, Physical examination, Physiotherapy, Range of movement, Whiplash injury, X-rays\"\n",
            "  ],\n",
            "  \"Summary\": \"Patient had a car accident on September 1st. Another car hit her from behind and pushed her car into the one in front. She hit her head on the steering wheel. She had to take painkillers and physiotherapy to help with the stiffness and discomfort. She still gets occasional backaches, but it's nothing like before. She took a week off work and is back to her usual routine. She should make a full recovery within six months of the accident. There are no signs of long-term damage or degeneration. If anything changes, she can always come back for a follow-up.\"\n",
            "}\n",
            "\n",
            " GENERATED SENTIMENT ANALYSIS : \n",
            "\n",
            "{\n",
            "  \"Sentiment\": \"Anxious\",\n",
            "  \"Intent\": \"Reporting symptoms\"\n",
            "}\n",
            "\n",
            " GENERATED SOAP NOTE : \n",
            "\n",
            "{\n",
            "  \"Subjective\": {\n",
            "    \"Chief_Complaint\": \"Occasional backaches and discomfort following a car accident.\",\n",
            "    \"History_of_Present_Illness\": \"The patient was involved in a car accident on September 1st, around 12:30 PM, when her car was hit from behind, pushing it into the car in front. She was wearing her seatbelt. Immediately after the impact, she hit her head on the steering wheel and experienced pain in her neck and back. She sought medical attention at Moss Bank Accident and Emergency, where she was diagnosed with a whiplash injury and advised to go home without X-rays. The first four weeks post-accident were marked by severe neck and back pain, difficulty sleeping, and regular painkiller use. She subsequently completed ten sessions of physiotherapy. Currently, she experiences occasional backaches, which are significantly less severe than before. She denies anxiety while driving, difficulty concentrating, or any emotional issues related to the accident. She took one week off work but reports no ongoing impact on her work or daily life.\"\n",
            "  },\n",
            "  \"Objective\": {\n",
            "    \"Physical_Exam\": \"Physical examination revealed a full range of movement in the neck and back. There was no tenderness or signs of lasting damage. Muscles and spine appeared to be in good condition.\",\n",
            "    \"Observations\": \"\"\n",
            "  },\n",
            "  \"Assessment\": {\n",
            "    \"Diagnosis\": \"Whiplash injury, resolving.\",\n",
            "    \"Severity\": \"Mild residual discomfort, with significant improvement since the initial injury.\"\n",
            "  },\n",
            "  \"Plan\": {\n",
            "    \"Treatment\": \"No new treatment initiated. Patient has completed physiotherapy. Continued self-management as needed.\",\n",
            "    \"Follow_Up\": \"The patient is expected to make a full recovery within six months of the accident, with no anticipated long-term impact on work or daily life. She should return for a follow-up if symptoms change or worsen.\"\n",
            "  }\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    run_pipeline(TRANSCRIPT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "id": "G8XG6HFftCbZ",
      "metadata": {
        "id": "G8XG6HFftCbZ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}